{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from settings import *\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "from model import MyEmbedder\n",
    "from datetime import datetime\n",
    "from load import *\n",
    "\n",
    "args = {\n",
    "    'device':'cuda:0',\n",
    "    'time':datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
    "    'language':'ja_en',\n",
    "    'model_language':'ja_en',\n",
    "    'epoch':300,\n",
    "    'batch_size':64,\n",
    "    'queue_length':64,\n",
    "    'center_norm':False,\n",
    "    'neighbor_norm':True,\n",
    "    'emb_norm':True,\n",
    "    'combine':True,\n",
    "    'gat_num':1,\n",
    "    't': 0.08,\n",
    "    'momentum':0.9999,\n",
    "    'lr':1e-6,\n",
    "    'dropout':0.3\n",
    "}\n",
    "device = torch.device('cuda')\n",
    "path = \"/home/mrcactus/Thesis/ACEA/data/DBP15K/ja_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ill_idx = load_triples(path + \"/ref_ent_ids\", file_num=1) # ground truth\n",
    "rate, val = 0.3, 0.0\n",
    "ill_train_idx, ill_val_idx, ill_test_idx = np.array(ill_idx[:int(len(ill_idx) // 1 * rate)], dtype=np.int32), np.array(ill_idx[int(len(ill_idx) // 1 * rate) : int(len(ill_idx) // 1 * (rate+val))], dtype=np.int32), np.array(ill_idx[int(len(ill_idx) // 1 * (rate+val)):], dtype=np.int32)\n",
    "ill_train_idx = list(zip(*ill_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ill_test_idx[0][0]\n",
    "link = {}\n",
    "for [k, v] in ill_test_idx:\n",
    "    link[k] = v \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedset = SeedDataset(ill_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedloader = Data.DataLoader(\n",
    "            dataset=seedset,  # torch TensorDataset format\n",
    "            batch_size=64,  # all test data\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_batches = []\n",
    "i = 0\n",
    "for batch_id, (token_data, id_data) in enumerate(seedloader):\n",
    "    all_data_batches.append([torch.Tensor(list(zip(*token_data)))[0], \\\n",
    "                             torch.Tensor(list(zip(*id_data)))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = DBP15KRawNeighbors(path, 'ja_en', \"1\")\n",
    "loader2 = DBP15KRawNeighbors(path, 'ja_en', \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset1 = MyRawdataset(loader1.id_neighbors_dict, loader1.id_adj_tensor_dict)\n",
    "myset2 = MyRawdataset(loader2.id_neighbors_dict, loader2.id_adj_tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_data_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader1 = Data.DataLoader(\n",
    "            dataset=myset1,  # torch TensorDataset format\n",
    "            batch_size=64,  # all test data\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "eval_loader2 = Data.DataLoader(\n",
    "            dataset=myset2,  # torch TensorDataset format\n",
    "            batch_size=64,  # all test data\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyEmbedder(args, VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed=37):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "fix_seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=args['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "def cal_sim(v1, v2, link, ids_1, inverse_ids_2):\n",
    "    source = [_id for _id in ids_1 if _id in link]\n",
    "    target = np.array(\n",
    "        [inverse_ids_2[link[_id]] if link[_id] in inverse_ids_2 else 99999 for _id in source])\n",
    "    src_idx = [idx for idx in range(len(ids_1)) if ids_1[idx] in link]\n",
    "    v1 = np.concatenate(tuple(v1), axis=0)[src_idx, :]\n",
    "    v2 = np.concatenate(tuple(v2), axis=0)\n",
    "    index = faiss.IndexFlatIP(v2.shape[1])\n",
    "    index.add(np.ascontiguousarray(v2))\n",
    "    D, I = index.search(np.ascontiguousarray(v1), 10)\n",
    "    return source, target, D, I # D是相似性矩阵， I是ID矩阵\n",
    "def evaluate(model, eval_loader1, eval_loader2, link, step):\n",
    "    print(\"Evaluate at epoch {}...\".format(step))\n",
    "    ids_1, ids_2, vector_1, vector_2 = list(), list(), list(), list()\n",
    "    inverse_ids_2 = dict()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for sample_id_1, (token_data_1, id_data_1) in tqdm(enumerate(eval_loader1)):\n",
    "            entity_vector_1 = model(token_data_1).squeeze().detach().cpu().numpy()\n",
    "            ids_1.extend(id_data_1.squeeze().tolist())\n",
    "            vector_1.append(entity_vector_1)\n",
    "\n",
    "        for sample_id_2, (token_data_2, id_data_2) in tqdm(enumerate(eval_loader2)):\n",
    "            entity_vector_2 = model(token_data_2).squeeze().detach().cpu().numpy()\n",
    "            ids_2.extend(id_data_2.squeeze().tolist())\n",
    "            vector_2.append(entity_vector_2)\n",
    "\n",
    "    for idx, _id in enumerate(ids_2):\n",
    "        inverse_ids_2[_id] = idx\n",
    "    def cal_hit(v1, v2, link):\n",
    "        source, target, D, I = cal_sim(v1, v2, link, ids_1, inverse_ids_2)\n",
    "        print(D)\n",
    "        hit1 = (I[:, 0] == target).astype(np.int32).sum() / len(source)\n",
    "        hit10 = (I == target[:, np.newaxis]).astype(np.int32).sum() / len(source)\n",
    "        print(\"#Entity: {}\".format(len(source)))\n",
    "        print(\"Hit@1: {}\".format(round(hit1, 3)))\n",
    "        print(\"Hit@10:{}\".format(round(hit10, 3)))\n",
    "        return round(hit1, 3), round(hit10, 3)\n",
    "    print('===========Test===========')\n",
    "    print(\"len v1:\" + str(len(vector_1)))\n",
    "    hit1_test, hit10_test = cal_hit(vector_1, vector_2, link)\n",
    "    return hit1_test, hit10_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate at epoch 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "310it [00:01, 186.01it/s]\n",
      "310it [00:01, 205.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Test===========\n",
      "len v1:310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 16.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84657776 0.7415437  0.7158799  ... 0.68777066 0.68646765 0.68589044]\n",
      " [0.88217133 0.7332629  0.72316325 ... 0.6892617  0.68702865 0.68601644]\n",
      " [0.91740835 0.89709264 0.8920222  ... 0.8713179  0.8687122  0.86766076]\n",
      " ...\n",
      " [0.8061072  0.76103747 0.734789   ... 0.6914282  0.69129246 0.6908588 ]\n",
      " [0.8454975  0.79202276 0.78825045 ... 0.7599765  0.750582   0.74111545]\n",
      " [0.88704264 0.8526741  0.8517467  ... 0.8420168  0.8414339  0.8401853 ]]\n",
      "#Entity: 10500\n",
      "Hit@1: 0.79\n",
      "Hit@10:0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [00:04, 21.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 69 loss: 0.01619906537234783\n",
      "Evaluate at epoch 0: batch 69...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "310it [00:01, 216.77it/s]\n",
      "310it [00:01, 179.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Test===========\n",
      "len v1:310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:08,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8784205  0.8738854  0.8728077  ... 0.86947155 0.8633952  0.8631546 ]\n",
      " [0.62848747 0.61246824 0.6124333  ... 0.5788074  0.5685188  0.5635835 ]\n",
      " [0.8903419  0.87099636 0.8699853  ... 0.8631131  0.86285746 0.8619568 ]\n",
      " ...\n",
      " [0.5716382  0.5633813  0.5631603  ... 0.5536786  0.55255115 0.551604  ]\n",
      " [0.78205824 0.77389795 0.7531215  ... 0.73259354 0.7198845  0.71742606]\n",
      " [0.7528471  0.61708367 0.6132751  ... 0.6035112  0.60108244 0.5987027 ]]\n",
      "#Entity: 10500\n",
      "Hit@1: 0.793\n",
      "Hit@10:0.898\n",
      "Test Hit@1(10)    = 0.793(0.898) at epoch 0 batch 69\n",
      "Best Valid Hit@1  = 0(0) at epoch 0\n",
      "Best Valid Hit@10 = 0(0) at epoch 0\n",
      "Test @ Best Valid = 0(0) at epoch 0 batch 0\n",
      "Best Test  Hit@1  = 0.793(0.898) at epoch 0\n",
      "Best Test  Hit@10 = 0.898(0.793) at epoch 0\n",
      "====================================\n",
      "start: 2024-01-25 15:52:18\n",
      "end: 2024-01-25 15:52:31\n",
      "used_time: 0:00:13.195266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "start_time = datetime.now()\n",
    "evaluate(model, eval_loader1, eval_loader2, link, 0)\n",
    "best_hit1_valid_epoch = 0\n",
    "best_hit10_valid_epoch = 0\n",
    "best_hit1_test_epoch = 0\n",
    "best_hit10_test_epoch = 0\n",
    "best_hit1_valid = 0\n",
    "best_hit10_valid = 0\n",
    "best_hit1_valid_hit10 = 0\n",
    "best_hit10_valid_hit1 = 0\n",
    "best_hit1_test = 0\n",
    "best_hit10_test = 0\n",
    "best_hit1_test_hit10 = 0\n",
    "best_hit10_test_hit1 = 0\n",
    "record_hit1 = 0\n",
    "record_hit10 = 0\n",
    "record_epoch = 0\n",
    "record_batch_id = 0\n",
    "for epoch in range(1):\n",
    "    for batch_id, (x_ids, y_ids) in tqdm(enumerate(all_data_batches)):\n",
    "        kg1_train_ent_idx = list(map(lambda x: int(x), list(x_ids)))\n",
    "        kg1_train_ent_emb = None \n",
    "        kg2_train_ent_idx = list(map(lambda x: int(x), list(y_ids)))\n",
    "        kg2_train_ent_emb = None \n",
    "        with torch.no_grad():\n",
    "            for idx in kg1_train_ent_idx:\n",
    "                if kg1_train_ent_emb==None:\n",
    "                    kg1_train_ent_emb = myset1.id_emb[idx].unsqueeze(0)\n",
    "                else:\n",
    "                    kg1_train_ent_emb = torch.cat((kg1_train_ent_emb,\\\n",
    "                                                myset1.id_emb[idx].unsqueeze(0)),0)\n",
    "            for idx in kg2_train_ent_idx:\n",
    "                if kg2_train_ent_emb==None:\n",
    "                    kg2_train_ent_emb = myset2.id_emb[idx].unsqueeze(0)\n",
    "                else:\n",
    "                    kg2_train_ent_emb = torch.cat((kg2_train_ent_emb,\\\n",
    "                                                myset2.id_emb[idx].unsqueeze(0)),0)\n",
    "            # kg1_train_ent_emb.append(myset1.id_emb[idx])\n",
    "            idx = [i for i in range(kg2_train_ent_emb.size(0)-1,-1,-1)]\n",
    "            idx = torch.LongTensor(idx)\n",
    "            neg_queue = kg2_train_ent_emb.index_select(0, idx)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pos_1 = model(kg1_train_ent_emb)\n",
    "        pos_2 = model(kg2_train_ent_emb)\n",
    "        neg = model(neg_queue)\n",
    "        contrastive_loss = model.contrastive_loss(pos_1, pos_2, neg)\n",
    "\n",
    "        contrastive_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id == len(all_data_batches) - 1:\n",
    "        # if batch_id % 200 == 0 or batch_id == len(all_data_batches) - 1:\n",
    "            print('epoch: {} batch: {} loss: {}'.format(epoch, batch_id,\n",
    "                                                        contrastive_loss.detach().cpu().data / 64))\n",
    "            hit1_test, hit10_test = evaluate(model, eval_loader1, eval_loader2, link, str(epoch)+\": batch \"+str(batch_id))\n",
    "\n",
    "            if hit1_test > best_hit1_test:\n",
    "                best_hit1_test = hit1_test\n",
    "                best_hit1_test_hit10 = hit10_test\n",
    "                best_hit1_test_epoch = epoch\n",
    "            if hit10_test  > best_hit10_test:\n",
    "                best_hit10_test = hit10_test\n",
    "                best_hit10_test_hit1 = hit1_test\n",
    "                best_hit10_test_epoch = epoch\n",
    "            \n",
    "            print('Test Hit@1(10)    = {}({}) at epoch {} batch {}'.format(hit1_test, hit10_test, epoch, batch_id))\n",
    "            print('Best Valid Hit@1  = {}({}) at epoch {}'.format(best_hit1_valid, best_hit1_valid_hit10, best_hit1_valid_epoch))\n",
    "            print('Best Valid Hit@10 = {}({}) at epoch {}'.format(best_hit10_valid,best_hit10_valid_hit1, best_hit10_valid_epoch))\n",
    "            print('Test @ Best Valid = {}({}) at epoch {} batch {}'.format(record_hit1, record_hit10, record_epoch, record_batch_id))\n",
    "\n",
    "            print('Best Test  Hit@1  = {}({}) at epoch {}'.format(best_hit1_test, best_hit1_test_hit10, best_hit1_test_epoch))\n",
    "            print('Best Test  Hit@10 = {}({}) at epoch {}'.format(best_hit10_test,best_hit10_test_hit1, best_hit10_test_epoch))\n",
    "            print(\"====================================\")\n",
    "end_time = datetime.now()\n",
    "print(\"start: \"+start_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"end: \"+end_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"used_time: \"+ str(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfkg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
