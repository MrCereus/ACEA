{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import abspath, dirname, join, exists\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import *\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "\n",
    "data_dir = '/home/mrcactus/Thesis/ACEA/data'\n",
    "TOKEN_LEN = 50\n",
    "VOCAB_SIZE = 100000\n",
    "LaBSE_DIM = 768\n",
    "EMBED_DIM = 300\n",
    "BATCH_SIZE = 96\n",
    "FASTTEXT_DIM = 300\n",
    "NEIGHBOR_SIZE = 20 \n",
    "ATTENTION_DIM = 300\n",
    "MULTI_HEAD_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_options(parser):\n",
    "    parser.add_argument('--device', type=str, default='cuda:0')\n",
    "    parser.add_argument('--time', type=str, default=datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    parser.add_argument('--language', type=str, default='zh_en')\n",
    "    parser.add_argument('--model_language', type=str, default='zh_en')\n",
    "    parser.add_argument('--model', type=str, default='LaBSE')\n",
    "\n",
    "    parser.add_argument('--epoch', type=int, default=300)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--queue_length', type=int, default=64)\n",
    "\n",
    "    parser.add_argument('--center_norm', type=bool, default=False)\n",
    "    parser.add_argument('--neighbor_norm', type=bool, default=True)\n",
    "    parser.add_argument('--emb_norm', type=bool, default=True)\n",
    "    parser.add_argument('--combine', type=bool, default=True)\n",
    "\n",
    "    parser.add_argument('--gat_num', type=int, default=1)\n",
    "\n",
    "    parser.add_argument('--t', type=float, default=0.08)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9999)\n",
    "    parser.add_argument('--lr', type=float, default=1e-6)\n",
    "    parser.add_argument('--dropout', type=float, default=0.3)\n",
    "\n",
    "    return parser.parse_args()\n",
    "# parser = argparse.ArgumentParser()\n",
    "args = {\n",
    "    'device':'cuda:0',\n",
    "    'time':datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
    "    'language':'zh_en',\n",
    "    'model_language':'zh_en',\n",
    "    'epoch':300,\n",
    "    'batch_size':64,\n",
    "    'queue_length':64,\n",
    "    'center_norm':False,\n",
    "    'neighbor_norm':True,\n",
    "    'emb_norm':True,\n",
    "    'combine':True,\n",
    "    'gat_num':1,\n",
    "    't':0.08,\n",
    "    'momentum':0.9999,\n",
    "    'lr':1e-6,\n",
    "    'dropout':0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCESoftmaxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(NCESoftmaxLoss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.squeeze()\n",
    "        label = torch.zeros([batch_size]).to(self.device).long()\n",
    "        loss = self.criterion(x, label)\n",
    "        return loss\n",
    "class MyEmbedder(nn.Module):\n",
    "    def __init__(self, args, vocab_size, padding=ord(' ')):\n",
    "        super(MyEmbedder, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.device = torch.device(self.args['device'])\n",
    "\n",
    "        self.attn = BatchMultiHeadGraphAttention(self.device, self.args)\n",
    "        \n",
    "        self.attn_mlp = nn.Sequential(\n",
    "            nn.Linear(LaBSE_DIM * 2, LaBSE_DIM),\n",
    "        )\n",
    "\n",
    "        # loss\n",
    "        self.criterion = NCESoftmaxLoss(self.device)\n",
    "\n",
    "        # batch queue\n",
    "        self.batch_queue = []\n",
    "\n",
    "    def contrastive_loss(self, pos_1, pos_2, neg_value):\n",
    "        bsz = pos_1.shape[0]\n",
    "        l_pos = torch.bmm(pos_1.view(bsz, 1, -1), pos_2.view(bsz, -1, 1))\n",
    "        l_pos = l_pos.view(bsz, 1)\n",
    "        l_neg = torch.mm(pos_1.view(bsz, -1), neg_value.t())\n",
    "        logits = torch.cat((l_pos, l_neg), dim=1)\n",
    "        logits = logits.squeeze().contiguous()\n",
    "        return self.criterion(logits / self.args['t'])\n",
    "\n",
    "    def update(self, network: nn.Module):\n",
    "        for key_param, query_param in zip(self.parameters(), network.parameters()):\n",
    "            key_param.data *= self.args['momentum']\n",
    "            key_param.data += (1 - self.args['momentum']) * query_param.data\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = batch.to(self.device)\n",
    "        batch_in = batch[:, :, :LaBSE_DIM]\n",
    "        adj = batch[:, :, LaBSE_DIM:]\n",
    "\n",
    "        center = batch_in[:, 0].to(self.device)\n",
    "        center_neigh = batch_in.to(self.device)\n",
    "\n",
    "        for i in range(0, self.args['gat_num']):\n",
    "            center_neigh = self.attn(center_neigh, adj.bool()).squeeze(1)\n",
    "        \n",
    "        center_neigh = center_neigh[:, 0]\n",
    "\n",
    "        if self.args['center_norm']:\n",
    "            center = F.normalize(center, p=2, dim=1)\n",
    "        if self.args['neighbor_norm']:\n",
    "            center_neigh = F.normalize(center_neigh, p=2, dim=1)\n",
    "        if self.args['combine']:\n",
    "            out_hat = torch.cat((center, center_neigh), dim=1)\n",
    "            out_hat = self.attn_mlp(out_hat)\n",
    "            if self.args['emb_norm']:\n",
    "                out_hat = F.normalize(out_hat, p=2, dim=1)\n",
    "        else:\n",
    "            out_hat = center_neigh\n",
    "\n",
    "        return out_hat\n",
    "\n",
    "\n",
    "class BatchMultiHeadGraphAttention(nn.Module):\n",
    "    def __init__(self, device, args, n_head=MULTI_HEAD_DIM, f_in=LaBSE_DIM, f_out=LaBSE_DIM, bias=True):\n",
    "        super(BatchMultiHeadGraphAttention, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_head = n_head\n",
    "        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n",
    "        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n",
    "        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(f_out))\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        nn.init.xavier_uniform_(self.a_src)\n",
    "        nn.init.xavier_uniform_(self.a_dst)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        bs, n = h.size()[:2]  # h is of size bs x n x f_in\n",
    "        h_prime = torch.matmul(h.unsqueeze(1), self.w)  # bs x n_head x n x f_out\n",
    "        attn_src = torch.matmul(torch.tanh(h_prime), self.a_src)  # bs x n_head x n x 1\n",
    "        attn_dst = torch.matmul(torch.tanh(h_prime), self.a_dst)  # bs x n_head x n x 1\n",
    "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(0, 1, 3, 2)  # bs x n_head x n x n\n",
    "\n",
    "        attn = self.leaky_relu(attn)\n",
    "        mask = ~(adj.unsqueeze(1) | torch.eye(adj.shape[-1]).bool().to(self.device))  # bs x 1 x n x n\n",
    "        attn.data.masked_fill_(mask, float(\"-inf\"))\n",
    "        attn = self.softmax(attn)  # bs x n_head x n x n\n",
    "        attn = self.dropout(attn)\n",
    "        # logging.info(\"attn: \", attn)\n",
    "        # logging.info(\"attn.shape: \", attn.shape)\n",
    "        output = torch.matmul(attn, h_prime)  # bs x n_head x n x f_out\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mrcactus/Thesis/ACEA/data/DBP15K/ja_en'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = join(data_dir, 'DBP15K', 'ja_en')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(data_dir, file_num=2):\n",
    "    if file_num == 2:\n",
    "        file_names = [data_dir + str(i) for i in range(1, 3)]\n",
    "    else:\n",
    "        file_names = [data_dir]\n",
    "    what2id, id2what, ids = {}, {}, []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read().strip().split(\"\\n\")\n",
    "            data = [i.split(\"\\t\") for i in data]\n",
    "            what2id = {**what2id, **dict([[i[1], int(i[0])] for i in data])}\n",
    "            id2what = {**id2what, **dict([[int(i[0]), i[1]] for i in data])}\n",
    "            ids.append(set([int(i[0]) for i in data]))\n",
    "    return what2id, id2what, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path == \"/home/mrcactus/Thesis/ACEA/data/DBP15K/ja_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2id_dict, id2ent_dict, [kg1_ent_ids, kg2_ent_ids] = load_dict(path + \"/cleaned_ent_ids_\", file_num=2)\n",
    "# /home/mrcactus/Thesis/ACEA/data/DBP15K/ja_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id_dict, id2rel_dict, [kg1_rel_ids, kg2_rel_ids] = load_dict(path + \"/cleaned_rel_ids_\", file_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triples(data_dir, file_num=2):\n",
    "    if file_num == 2:\n",
    "        file_names = [data_dir + str(i) for i in range(1, 3)]\n",
    "    else:\n",
    "        file_names = [data_dir]\n",
    "    triple = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read().strip().split(\"\\n\")\n",
    "            data = [tuple(map(int, i.split(\"\\t\"))) for i in data]\n",
    "            triple += data\n",
    "    np.random.shuffle(triple)\n",
    "    return triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_idx = load_triples(path + \"/triples_\", file_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ill_idx = load_triples(path + \"/ref_ent_ids\", file_num=1) # ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LaBSE_emb(data_dir, file_num):\n",
    "    if file_num == 2:\n",
    "        file_names = [data_dir + str(i) + '.pkl' for i in range(1, 3)]\n",
    "    else:\n",
    "        file_names = [data_dir + '.pkl']\n",
    "    id_entity = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            id_entity.append(pickle.load(f))\n",
    "    return id_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "[kg1_ids_ent_emb, kg2_ids_ent_emb] = load_LaBSE_emb(path + \"/raw_LaBSE_emb_\", file_num = 2)\n",
    "# kg1_ids_ent_emb的格式：\n",
    "# kg1_ids_ent_emb[i] = [[emb]]，二维数组，取的时候要在后面加[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8857]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = util.pytorch_cos_sim(kg1_ids_ent_emb[ill_idx[0][0]],\n",
    "                      kg2_ids_ent_emb[ill_idx[0][1]])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, val = 0.2, 0.1\n",
    "ill_train_idx, ill_val_idx, ill_test_idx = np.array(ill_idx[:int(len(ill_idx) // 1 * rate)], dtype=np.int32), np.array(ill_idx[int(len(ill_idx) // 1 * rate) : int(len(ill_idx) // 1 * (rate+val))], dtype=np.int32), np.array(ill_idx[int(len(ill_idx) // 1 * (rate+val)):], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ill_train_idx = list(zip(*ill_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过多个key，批量从dict中获取value\n",
    "# 这里是获取ground truth中实体id对应的emb\n",
    "kg1_train_ent_idx = ill_train_idx[0]\n",
    "kg2_train_ent_idx = ill_train_idx[1]\n",
    "# kg1_ids_ent_emb\n",
    "# kg1_train_ent_idx\n",
    "# from operator import itemgetter\n",
    "# kg1_train_ent_emb = itemgetter(*kg1_train_ent_idx)(kg1_ids_ent_emb)\n",
    "# kg2_train_ent_emb = itemgetter(*kg2_train_ent_idx)(kg2_ids_ent_emb)\n",
    "\n",
    "kg1_train_ent_emb = []\n",
    "kg2_train_ent_emb = []\n",
    "for idx in kg1_train_ent_idx:\n",
    "    kg1_train_ent_emb.append(kg1_ids_ent_emb[idx][0])\n",
    "for idx in kg2_train_ent_idx:\n",
    "    kg2_train_ent_emb.append(kg2_ids_ent_emb[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8857, 0.6664, 0.5875,  ..., 0.5349, 0.6056, 0.5527],\n",
       "        [0.6301, 0.7858, 0.5265,  ..., 0.4918, 0.6622, 0.5003],\n",
       "        [0.5795, 0.6063, 0.9310,  ..., 0.6285, 0.5670, 0.5919],\n",
       "        ...,\n",
       "        [0.5152, 0.5741, 0.5957,  ..., 0.8379, 0.6220, 0.5366],\n",
       "        [0.5934, 0.6929, 0.5839,  ..., 0.6201, 0.8585, 0.5554],\n",
       "        [0.6172, 0.5440, 0.6283,  ..., 0.5713, 0.5970, 0.9448]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = []\n",
    "s = util.pytorch_cos_sim(kg1_train_ent_emb,kg2_train_ent_emb)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = MyEmbedder(args, VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "lr = args['lr']\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed=37):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_992952/2593595691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfix_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpos_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkg1_train_ent_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/selfkg/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_992952/156902891.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mbatch_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mLaBSE_DIM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLaBSE_DIM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "fix_seed(37)\n",
    "optimizer.zero_grad()\n",
    "pos_1 = model(torch.tensor(kg1_train_ent_emb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfkg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
